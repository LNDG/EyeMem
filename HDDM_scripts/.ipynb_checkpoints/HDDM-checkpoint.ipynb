{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: use py36 virtual environment to run this! -source activate py36\n",
    "# but use Python 3 kernel! NOT the python 3.6 \n",
    "\n",
    "# check if right version is being used\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kabuki\n",
    "import platform\n",
    "from kabuki.analyze import gelman_rubin\n",
    "import scipy.io as sio\n",
    "\n",
    "%matplotlib inline\n",
    "import hddm\n",
    "print(hddm.__version__)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv file into a NumPy structured array\n",
    "def load_hddm_csv(path):\n",
    "    data = hddm.load_csv(path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em1_hddm_study = load_hddm_csv('/Users/terlau/LNDG/EyeMem/EyeMem_1/HDDM/EyeMem_hddm_study.csv')\n",
    "em1_hddm_test = load_hddm_csv('/Users/terlau/LNDG/EyeMem/EyeMem_1/HDDM/EyeMem_hddm_test.csv')\n",
    "em2_hddm_study = load_hddm_csv('/Users/terlau/LNDG/EyeMem/EyeMem_2/behavior/plotting/EyeMem_hddm_study.csv')\n",
    "em2_hddm_test = load_hddm_csv('/Users/terlau/LNDG/EyeMem/EyeMem_2/behavior/plotting/EyeMem_hddm_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em2_hddm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for HDDM StimCoding\n",
    "Fitting the DDM to RT distributions for the two choice categories, conditioned on the stimulus category for each trial - this is referred to as ‘stimulus coding’. This fitting method deviates from a widely used expression of the model, where RT distributions for correct and incorrect choices are fit (also called ‘accuracy coding’). Only the stimulus coding can fit decision biases towards one choice over the other.\n",
    "\n",
    "In stimulus coding 1’s and 0’s correspond to the choice (e.g. categorization of the stimulus). HDDM interprets 0 and 1 responses as lower and upper boundary responses, respectively, so in principle either of these schemes (acc or stim coding) is valid.\n",
    "\n",
    "This StimCoding model expects data to have a column named stim with two distinct identifiers. For identifier 1, drift-rate v will be used while for identifier 0, -v will be used. So ultimately you only estimate one drift-rate. Alternatively you can use bias z and 1-z if you set split_param='z'. See the HDDMStimCoding help doc for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, study):\n",
    "    \n",
    "    appended_data = []\n",
    "    # create a lst with unique sub ids\n",
    "    SubjectList = np.unique(df['subj_idx'])\n",
    "    df.rename(columns={'response':'key'}, inplace=True)\n",
    "\n",
    "    for sub in SubjectList:\n",
    "\n",
    "        sub_df = df[df['subj_idx'] == sub]\n",
    "        # correct & stim was present, hit --> finding response key corresponding to \"old\"\n",
    "        resp = np.unique(sub_df[\"key\"][(sub_df[\"stim\"] == 1) & (sub_df[\"accuracy\"] == 1)])\n",
    "\n",
    "        # 1 (old) and 0 (new) where response key was indicating presence of stim, so subject said \"yes I've seen this\", 1 i.e. old, and 0 (new) otherwise\n",
    "        sub_df['response']=np.where(sub_df[\"key\"] == resp[0],1,0)\n",
    "    \n",
    "        # store DataFrame in list\n",
    "        appended_data.append(sub_df)\n",
    "\n",
    "\n",
    "    data = pd.concat(appended_data)\n",
    "    plot_RT_dist(data, study)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "em1_hddm_study_prep = prepare_data(em1_hddm_study, ' EM1 Study')\n",
    "em1_hddm_test_prep = prepare_data(em1_hddm_test, ' EM1 Test')\n",
    "em2_hddm_study_prep = prepare_data(em2_hddm_study, ' EM2 Study')\n",
    "em2_hddm_test_prep = prepare_data(em2_hddm_test, ' EM2 Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert stim column from int to bool - why??\n",
    "#final_data['stim'] = final_data['stim'].astype(bool)\n",
    "# for stimcoding, the two identities should be 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do: figure out if part of RT data cleaning should be keeping only trials within +3 to -3 standard deviations. Had this before but apparently for some reason we decided against it, with Niels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_RT_dist(data, title):\n",
    "    #clean from invalid RT times / nan vals\n",
    "    data = data.dropna() # this maybe better: data=df.loc[~df['rt'].isnull()]\n",
    "    #data['rt'] = data[data[\"rt\"] > 1.65] # Remove trials greater than 1650ms\n",
    "    #data['z_rt'] = stats.zscore(data['rt'])\n",
    "    # keep only trials within +3 to -3 standard deviations\n",
    "    #data = data[np.abs(data['z_rt']) < 3]\n",
    "    #data = data[np.abs(data['rt']-data['rt'].mean()) <= (3*data['rt'].std())]\n",
    "\n",
    "    data = data[data.rt > 0.2] # drop too fast RT's\n",
    "    data = hddm.utils.flip_errors(data)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, xlabel='RT', ylabel='count', title='RT distributions'+title)\n",
    "    for i, subj_data in data.groupby('subj_idx'):\n",
    "        subj_data.rt.hist(bins=25, histtype='step', ax=ax)\n",
    "\n",
    "    #plt.savefig('RTdistribution_study_eyemem1.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running HDDM for both age groups, only gives one fit estimate\n",
    "# maybe better to run seperately, one model per group\n",
    "def get_age_group(df):\n",
    "    dfs = [x for _, x in df.groupby('age')]\n",
    "    YA = dfs[0]\n",
    "    OA = dfs[1]\n",
    "    return YA, OA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em1_hddm_study_YA, em1_hddm_study_OA = get_age_group(em1_hddm_study_prep)\n",
    "em1_hddm_test_YA, em1_hddm_test_OA = get_age_group(em1_hddm_test_prep)\n",
    "em2_hddm_study_YA, em2_hddm_study_OA = get_age_group(em2_hddm_study_prep)\n",
    "em2_hddm_test_YA, em2_hddm_test_OA = get_age_group(em2_hddm_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_standardmodel(data,group):\n",
    "\n",
    "    m = hddm.HDDM(data, depends_on={'v':'stim', 'a':'stim', 't':'stim'}) # , include='all'\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn=2500, dbname='/Users/terlau/LNDG/EyeMem/HDDM/db_standard%i'%group, db='pickle')\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit HDDM, multiple chains in parallel\n",
    "# run chains in parallel inc. bias models\n",
    "def run_biasmodel(data, group): #id\n",
    "    # group represent age group but also which study so em1 test YA for example\n",
    "    # response column represent the choice, old or new\n",
    "    # so shouldn't I use response for the stim column???\n",
    "    m = hddm.HDDMStimCoding(data, stim_col='stim', split_param='v', drift_criterion=True, bias=True, p_outlier=0.05) #  depends_on={'v':'stim', 'a':'stim', 't':'stim', 'dc':'stim', 'z':'stim' }, include='all'\n",
    "    # find a good starting point which helps with the convergence.\n",
    "    m.find_starting_values()\n",
    "    m.sample(5000, burn=2500, dbname='/Users/terlau/LNDG/EyeMem/HDDM/db_bias'+group+'.db', db='pickle') #%i'%id\n",
    "    #m.save('/Users/terlau/LNDG/EyeMem/HDDM/db_bias'+group+'.db')\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_posteriors()\n",
    "m.plot_posterior_predictive(columns=5,figsize=(14,12))\n",
    "#plt.savefig('/Users/terlau/LNDG/EyeMem/HDDM/plots/posterior_predictive_eyemem1_study_bias_model_OA.png')\n",
    "stats = m.gen_stats()\n",
    "#print(stats)\n",
    "#stats.to_csv('eyemem1_study_params_biasmodel_YA.csv' )\n",
    "#stats[stats.index.isin(['a', 'a_std', 'a_subj.11009', 'a_subj.11012'])]\n",
    "#m.plot_posteriors(['a', 't', 'v', 'a_std'])\n",
    "\n",
    "#saving the trace and the loading that trace into a new model.\n",
    "#m.mcmc(dbname='test.db')\n",
    "#m.sample(5000)\n",
    "# later point in time\n",
    "#m = hddm.HDDM # reconstruct model\n",
    "#m.load_from_db(dbname='test.db') # load traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = hddm.load('bias_model_YA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(kabuki)\n",
    "#importlib.reload(hddm)\n",
    "# need this for my adjustments in the source code to be implemented here without having to restart the kernel\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.parallel import Client\n",
    "v = Client()[:]\n",
    "\n",
    "# run model\n",
    "jobs = v.map(run_standardmodel, range(15)) # 4 is the number of CPUs\n",
    "\n",
    "models = jobs.get()\n",
    "\n",
    "a = gelman_rubin(models)\n",
    "b = pd.DataFrame.from_dict(a, orient='index')\n",
    "b.to_csv('gelman_rubin_vals_drop_lowdprime.csv')\n",
    "\n",
    "# Create a new model that has all traces concatenated\n",
    "# of individual models.\n",
    "m = kabuki.utils.concat_models(models)\n",
    "\n",
    "#%% export data\n",
    "m.save('hddmmodel_standardmodel_drop_lowdprime') # save to file\n",
    "\n",
    "test = m.gen_stats()\n",
    "test.to_csv('params_standardmodel_drop_lowdprime.csv' )\n",
    "\n",
    "#%% plotting and model fit checks\n",
    "# a = m.plot_posteriors_conditions()\n",
    "# plt.savefig('plot_posteriors_conditions.pdf')\n",
    "# m.plot_posteriors(['a', 't', 'v', 'dc', 'z'])\n",
    "\n",
    "# m.plot_posterior_predictive(figsize=(100, 50), ) # bins=1000\n",
    "\n",
    "m.plot_posterior_predictive(figsize=(100, 50), value_range= np.linspace(-1.5, 1.5, 10)) # bins=1000\n",
    "\n",
    "# # m.plot_posterior_quantiles(samples=3, columns=3, figsize=(100, 50))\n",
    "# plt.show()\n",
    "# plt.savefig('modelfitsHDDMbiasmodel_late.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
